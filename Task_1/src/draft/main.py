# ===============================================================
# LAB WORK: Analysis of the Adult Income Dataset (adult.data.csv)
# ===============================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from scipy.stats import chi2_contingency
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
)

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
plt.style.use("seaborn-v0_8")
sns.set_palette("Set2")
os.makedirs("plots", exist_ok=True)

# ---------- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
print("=" * 70)
print("üìò –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
print("=" * 70)
df = pd.read_csv("../../TheTask/adult.data.csv")

print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
print("\nüîπ –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
print(df.head())

# ---------- 2. –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
print("\n" + "=" * 70)
print("üßπ –û–ß–ò–°–¢–ö–ê –î–ê–ù–ù–´–•")
print("=" * 70)

# –ó–∞–º–µ–Ω—è–µ–º '?' –Ω–∞ NaN
df.replace("?", np.nan, inplace=True)
print("\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º:")
print(df.isna().sum())

# –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
df.dropna(inplace=True)
print(f"\n‚úÖ –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {df.shape[0]} —Å—Ç—Ä–æ–∫ –æ—Å—Ç–∞–ª–æ—Å—å.")

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
for col in df.select_dtypes("object").columns:
    df[col] = df[col].astype("category")

print("\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:")
print(df.dtypes)

# ---------- 3. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ ----------
print("\n" + "=" * 70)
print("üîç –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–°–ö–ò–ô –ê–ù–ê–õ–ò–ó")
print("=" * 70)

print("\nüîπ –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:")
print(df[["age", "fnlwgt", "hours-per-week"]].describe())

# –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
for col in ["age", "hours-per-week"]:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=20)
    plt.title(f"Distribution of {col}")
    plt.savefig(f"plots/{col}_hist.png")
    plt.close()

# Boxplot: —á–∞—Å—ã —Ä–∞–±–æ—Ç—ã –ø–æ —É—Ä–æ–≤–Ω—é –¥–æ—Ö–æ–¥–∞
plt.figure(figsize=(7, 5))
sns.boxplot(x="salary", y="hours-per-week", data=df)
plt.title("Work Hours by Income Level")
plt.savefig("plots/hours_by_income.png")
plt.close()

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
plt.figure(figsize=(10, 5))
sns.countplot(y="education", data=df, order=df["education"].value_counts().index)
plt.title("Education Distribution")
plt.savefig("plots/education_distribution.png")
plt.close()

# ---------- 4. –ü—Ä–æ—Å—Ç–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è ----------
print("\n" + "=" * 70)
print("üîé –ü–†–û–°–¢–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø")
print("=" * 70)

print("\n–î–æ—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
print(df["salary"].value_counts())

print("\n–î–æ—Ö–æ–¥ –ø–æ –ø–æ–ª—É:")
income_by_gender = df.groupby(["sex", "salary"]).size().unstack(fill_value=0)
print(income_by_gender)

# ---------- 5. –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö ----------
print("\n" + "=" * 70)
print("üßæ –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–ï–†–ï–ú–ï–ù–ù–´–•")
print("=" * 70)

cat_cols = ["workclass", "education", "marital-status", "occupation", "native-country"]
for col in cat_cols:
    print(f"{col}: {df[col].nunique()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

# ---------- 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑ ----------
print("\n" + "=" * 70)
print("üìä –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ï –¢–ï–°–¢–´")
print("=" * 70)

# –ü—Ä–∏–º–µ—Ä: —Å–≤—è–∑—å –º–µ–∂–¥—É –ø–æ–ª–æ–º –∏ –¥–æ—Ö–æ–¥–æ–º
contingency = pd.crosstab(df["sex"], df["salary"])
chi2, p, dof, expected = chi2_contingency(contingency)
print("\nChi-square test (sex vs salary):")
print(f"chi2 = {chi2:.3f}, p-value = {p:.6f}")

if p < 0.05:
    print("‚úÖ –†–∞–∑–Ω–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º–∞ (p < 0.05)")
else:
    print("‚ùå –†–∞–∑–Ω–∏—Ü–∞ –Ω–µ–∑–Ω–∞—á–∏–º–∞ (p >= 0.05)")

# ---------- 7. –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–∞ ----------
print("\n" + "=" * 70)
print("ü§ñ –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–ï –î–û–•–û–î–ê (>50K)")
print("=" * 70)

# –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
X = df.drop("salary", axis=1)
y = df["salary"]

cat_features = X.select_dtypes("category").columns
num_features = X.select_dtypes("number").columns

# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
preprocessor = ColumnTransformer(
    transformers=[("cat", OneHotEncoder(handle_unknown="ignore"), cat_features)],
    remainder="passthrough",
)

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- –ú–æ–¥–µ–ª—å 1: –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è ---
log_model = Pipeline(
    steps=[("preprocess", preprocessor), ("model", LogisticRegression(max_iter=500))]
)
log_model.fit(X_train, y_train)
y_pred = log_model.predict(X_test)

print("\nüîπ Logistic Regression Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# --- –ú–æ–¥–µ–ª—å 2: –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å ---
rf_model = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("model", RandomForestClassifier(n_estimators=200, random_state=42)),
    ]
)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

print("\nüîπ Random Forest Report:")
print(classification_report(y_test, rf_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, rf_pred))
print(f"Accuracy: {accuracy_score(y_test, rf_pred):.3f}")

# ---------- 8. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ----------
print("\n" + "=" * 70)
print("üéØ –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í (Random Forest)")
print("=" * 70)

# –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ—Å–ª–µ One-Hot Encoding
ohe = rf_model.named_steps["preprocess"].named_transformers_["cat"]
cat_feature_names = ohe.get_feature_names_out(cat_features)
all_features = np.concatenate([cat_feature_names, num_features])

importances = rf_model.named_steps["model"].feature_importances_
feat_imp = pd.Series(importances, index=all_features).sort_values(ascending=False)[:15]
print(feat_imp)

plt.figure(figsize=(8, 5))
sns.barplot(x=feat_imp.values, y=feat_imp.index)
plt.title("Top 15 Feature Importances")
plt.savefig("plots/feature_importance.png")
plt.close()

# ---------- 9. –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑—Ä—ã–≤–∞ –¥–æ—Ö–æ–¥–æ–≤ ----------
print("\n" + "=" * 70)
print("üí∞ –ê–ù–ê–õ–ò–ó –î–û–•–û–î–ù–´–• –†–ê–ó–†–´–í–û–í")
print("=" * 70)

income_gap = df.groupby(["sex", "race"])["salary"].value_counts(normalize=True).unstack()
print(income_gap)

# ---------- 10. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ ----------
print("\n" + "=" * 70)
print("üîÆ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –î–õ–Ø –ù–û–í–´–• –î–ê–ù–ù–´–•")
print("=" * 70)

new_data = pd.DataFrame({
    "age": [29],
    "workclass": ["Private"],
    "fnlwgt": [190000],
    "education": ["Bachelors"],
    "education-num": [13],
    "marital-status": ["Never-married"],
    "occupation": ["Tech-support"],
    "relationship": ["Not-in-family"],
    "race": ["White"],
    "sex": ["Female"],
    "capital-gain": [0],
    "capital-loss": [0],
    "hours-per-week": [40],
    "native-country": ["United-States"]
})
print("–ù–æ–≤–∞—è –∑–∞–ø–∏—Å—å:")
print(new_data)
print("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –¥–æ—Ö–æ–¥:", rf_model.predict(new_data)[0])

print("\n‚úÖ –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞!")
print("–í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É: plots/")
